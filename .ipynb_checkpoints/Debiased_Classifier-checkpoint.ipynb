{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZKukJx_2Kpj"
   },
   "outputs": [],
   "source": [
    "#Relevant Links\n",
    "#1. https://towardsdatascience.com/bert-classifier-just-another-pytorch-model-881b3cf05784\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "colab_type": "code",
    "id": "WpWg_DCjEpp3",
    "outputId": "b6e64e8e-81c1-489b-99a7-f2b43f258c8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: constant in /usr/local/lib/python3.6/dist-packages (0.0.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from constant) (2.10.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->constant) (1.1.1)\n",
      "Requirement already satisfied: bert-pytorch in /usr/local/lib/python3.6/dist-packages (0.0.1a4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from bert-pytorch) (4.28.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bert-pytorch) (1.17.4)\n",
      "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from bert-pytorch) (1.3.1)\n",
      "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
      "Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.17.4)\n",
      "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.3.1)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.10.32)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.11.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.32 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.13.32)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.11.28)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.32->boto3->pytorch-pretrained-bert) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.32->boto3->pytorch-pretrained-bert) (2.6.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.32->boto3->pytorch-pretrained-bert) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install constant\n",
    "!pip install bert-pytorch\n",
    "!pip install pytorch-pretrained-bert pytorch-nlp\n",
    "!pip install -U -q PyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ci2rw6qvDozu"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, recall_score, precision_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from google.colab import drive\n",
    "\n",
    "sys.path.append(os.path.join(os.path.dirname(sys.path[0]), 'analysis'))\n",
    "sys.path.append(os.path.join(os.path.dirname(os.path.dirname(sys.path[0])), 'configs' ))\n",
    "\n",
    "import constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65
    },
    "colab_type": "code",
    "id": "XoiPduk2GfSo",
    "outputId": "afb1e298-30a8-4f29-abe8-242ef279a4f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tesla P100-PCIE-16GB'"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam, BertModel\n",
    "from pytorch_pretrained_bert import BertConfig\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# specify GPU device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "id": "z6TTAVNku2Kc",
    "outputId": "ddae7167-8849-46aa-8e96-5521bfd29429"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#Data Reading\n",
    "# Mount drive for data reading\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')\n",
    "path = \"/content/drive/Shared drives/CIS 519 Project/Code/Dataset/Resampled4/\"\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "colab_type": "code",
    "id": "NIMzTTbsNNOA",
    "outputId": "c56f28b1-ce5c-4927-a882-5316bc474d98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec  8 17:32:43 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   32C    P0    25W / 250W |     10MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sj4bkVjlu8Kq"
   },
   "outputs": [],
   "source": [
    "#Read full data path = \"/content/drive/Shared drives/CIS 519 Project/Code/Dataset/\"\n",
    "df_train_total = pd.read_csv(path + 'train.csv')\n",
    "df_val_total = pd.read_csv(path + 'validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 870
    },
    "colab_type": "code",
    "id": "DwkdWl9Xu8O2",
    "outputId": "2d320572-d968-42f5-ccf7-dffe69a1758b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>christian</th>\n",
       "      <th>jewish</th>\n",
       "      <th>muslim</th>\n",
       "      <th>black</th>\n",
       "      <th>white</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "      <th>created_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5367475</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>This is an insult to all immigrants who took t...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2017-06-06 17:53:11.976493+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>389581</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>Eric, I was unaware that Obama had a line of p...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2016-07-15 23:22:57.603863+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>259203</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>\"malign her character because of alleged illeg...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2016-03-26 19:14:41.102508+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6274158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Couldn't agree more with your last sentence, E...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-11-02 15:49:53.222384+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5435081</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>The liberal, married, gay Black woman who took...</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2017-06-18 13:26:29.297687+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5693877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Get over it, two men took to the ring and one ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-08-01 02:59:30.400500+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5801144</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>Yes, yes, it's \"The Media\" who forced Trump to...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2017-08-19 16:02:37.201001+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5078252</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>\"cannon fodder\" - they are \"acceptable losses\"...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2017-03-31 18:03:36.139268+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5277082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No, not who cares, as in who cares, rather htt...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-05-19 14:33:37.520110+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>489731</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>The hate of white males from the progressive p...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>2016-09-24 04:44:47.907794+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    target  ... toxicity_annotator_count                   created_date\n",
       "0  5367475  0.500000  ...                       10  2017-06-06 17:53:11.976493+00\n",
       "1   389581  0.166667  ...                        6  2016-07-15 23:22:57.603863+00\n",
       "2   259203  0.600000  ...                       10  2016-03-26 19:14:41.102508+00\n",
       "3  6274158  0.000000  ...                        4  2017-11-02 15:49:53.222384+00\n",
       "4  5435081  0.500000  ...                       10  2017-06-18 13:26:29.297687+00\n",
       "5  5693877  0.000000  ...                        4  2017-08-01 02:59:30.400500+00\n",
       "6  5801144  0.600000  ...                       10  2017-08-19 16:02:37.201001+00\n",
       "7  5078252  0.500000  ...                       10  2017-03-31 18:03:36.139268+00\n",
       "8  5277082  0.000000  ...                        4  2017-05-19 14:33:37.520110+00\n",
       "9   489731  0.828571  ...                       70  2016-09-24 04:44:47.907794+00\n",
       "\n",
       "[10 rows x 15 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_total.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 755
    },
    "colab_type": "code",
    "id": "E7fHVdLas2hu",
    "outputId": "88b40769-2bef-4702-dc8a-f67c7257630f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>christian</th>\n",
       "      <th>jewish</th>\n",
       "      <th>muslim</th>\n",
       "      <th>black</th>\n",
       "      <th>white</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "      <th>created_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>400154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Donald always has been a lefty wing Manhattani...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2016-07-24 22:43:28.495610+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5092755</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Again, to be fair, it was just the one guy who...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-04-04 20:08:40.823984+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>797853</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>I am going to start an organization so ALL ord...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2017-01-06 04:54:33.684047+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5046660</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>LMAO...what on god's green earth makes you thi...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-03-23 23:54:30.927369+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>882416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Master Yoda told me; Hmmmm...Hmmm.. \"Much whin...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-01-23 04:17:54.377161+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>984833</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>I am SO happy and grateful to see people stand...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2017-02-12 07:08:51.852690+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5499900</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>Trump has been objectifying women for decades....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2017-06-29 19:30:21.743759+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>728632</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Ms. Kurdi claims that Canada is partly respons...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2016-12-19 15:24:50.573840+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5408070</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>The Catholic church confuses forgiveness with ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2017-06-13 18:26:08.994210+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5253827</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"Cultural Appropriation\" as an accusation is a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-05-14 02:35:25.722340+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    target  ... toxicity_annotator_count                   created_date\n",
       "0   400154  0.000000  ...                        4  2016-07-24 22:43:28.495610+00\n",
       "1  5092755  0.000000  ...                        4  2017-04-04 20:08:40.823984+00\n",
       "2   797853  0.600000  ...                       10  2017-01-06 04:54:33.684047+00\n",
       "3  5046660  1.000000  ...                        4  2017-03-23 23:54:30.927369+00\n",
       "4   882416  0.000000  ...                        4  2017-01-23 04:17:54.377161+00\n",
       "5   984833  0.166667  ...                        6  2017-02-12 07:08:51.852690+00\n",
       "6  5499900  0.166667  ...                        6  2017-06-29 19:30:21.743759+00\n",
       "7   728632  0.500000  ...                       10  2016-12-19 15:24:50.573840+00\n",
       "8  5408070  0.800000  ...                       10  2017-06-13 18:26:08.994210+00\n",
       "9  5253827  0.000000  ...                        4  2017-05-14 02:35:25.722340+00\n",
       "\n",
       "[10 rows x 15 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_total.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vSGr1TEOu8SY"
   },
   "outputs": [],
   "source": [
    "# comments = pd.DataFrame(df.comment_text)\n",
    "# #comments.to_csv(path + \"only_comments.csv\")\n",
    "# comments_text = df.comment_text\n",
    "# df_store = df.copy()\n",
    "# comments_store = comments.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Udi8XjuExtPO"
   },
   "outputs": [],
   "source": [
    "df_train = df_train_total#.sample(5000)\n",
    "df_val = df_val_total#.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0glp8vl8s0WA"
   },
   "outputs": [],
   "source": [
    "comments_train = df_train.comment_text\n",
    "comments_val = df_val.comment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M_0vTV0tvG6Q"
   },
   "outputs": [],
   "source": [
    "#Generate protected attribute - gender\n",
    "def extract_female_gender(x):\n",
    "  if np.isnan(x.female) or x.female < 0.5:\n",
    "      return 0\n",
    "  else:\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8946xw4GTSsk"
   },
   "outputs": [],
   "source": [
    "#Generate unprotected attribute labels\n",
    "def get_unprotected_class(list_of_protected):\n",
    "  new = [1 if i == 0 else 0 for i in list_of_protected]\n",
    "  return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UnWIvZxmvG86"
   },
   "outputs": [],
   "source": [
    "#Calculate metrics\n",
    "def get_metrics(labels, preds):\n",
    "  pred_flat = preds.flatten()\n",
    "  labels_flat = labels.flatten()\n",
    "\n",
    "  acc = accuracy_score(labels_flat, pred_flat)\n",
    "  pre = precision_score(labels_flat, pred_flat)\n",
    "  rec = recall_score(labels_flat, pred_flat)\n",
    "  f1 = f1_score(labels_flat, pred_flat, average=\"weighted\")\n",
    "\n",
    "  return acc, pre, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LInCuFiEvG_m"
   },
   "outputs": [],
   "source": [
    "#Generate labels\n",
    "toxicity_labels_train = list(df_train.target.apply(lambda x: 1 if x >= 0.5 else 0))\n",
    "identity_labels_train = list(df_train.apply(extract_female_gender, axis = 1))\n",
    "toxicity_labels_val = list(df_val.target.apply(lambda x: 1 if x >= 0.5 else 0))\n",
    "identity_labels_val = list(df_val.apply(extract_female_gender, axis = 1))\n",
    "unprotected_labels_train = get_unprotected_class(identity_labels_train)\n",
    "unprotected_labels_val = get_unprotected_class(identity_labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "id": "Yj7CCUdFvHCt",
    "outputId": "799e0620-e5b5-46cf-f89f-515e5e4d1e40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10974 10974\n",
      "0    This is an insult to all immigrants who took t...\n",
      "1    Eric, I was unaware that Obama had a line of p...\n",
      "2    \"malign her character because of alleged illeg...\n",
      "3    Couldn't agree more with your last sentence, E...\n",
      "4    The liberal, married, gay Black woman who took...\n",
      "5    Get over it, two men took to the ring and one ...\n",
      "6    Yes, yes, it's \"The Media\" who forced Trump to...\n",
      "7    \"cannon fodder\" - they are \"acceptable losses\"...\n",
      "8    No, not who cares, as in who cares, rather htt...\n",
      "9    The hate of white males from the progressive p...\n",
      "Name: comment_text, dtype: object\n",
      "[1, 0, 1, 0, 1, 0, 1, 1, 0, 1]\n",
      "[1, 0, 1, 0, 1, 0, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(len(comments_train), len(toxicity_labels_train))\n",
    "print(comments_train[:10])\n",
    "print(toxicity_labels_train[:10])\n",
    "print(identity_labels_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qfjrbmoUvcQx"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 128\n",
    "SEED = 519\n",
    "BATCH_SIZE = 32\n",
    "BERT_MODEL_PATH = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A_3512JxvcUU"
   },
   "outputs": [],
   "source": [
    "def convert_lines(example, max_seq_length,tokenizer):\n",
    "    #max_seq_length -=2\n",
    "    all_tokens = []\n",
    "    longer = 0\n",
    "    for text in tqdm(example):\n",
    "        tokens_a = tokenizer.tokenize(text)\n",
    "        if len(tokens_a)>max_seq_length:\n",
    "            tokens_a = tokens_a[:max_seq_length]\n",
    "            longer += 1\n",
    "        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
    "        all_tokens.append(one_token)\n",
    "    print(\"Tokens longer than max_length: \", longer)\n",
    "    return np.array(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "colab_type": "code",
    "id": "OM1WMxLVvcXt",
    "outputId": "a8699cbc-a6c6-4225-a4de-f037e35cca68"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10974/10974 [00:11<00:00, 971.19it/s]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens longer than max_length:  2690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:02<00:00, 930.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens longer than max_length:  544\n"
     ]
    }
   ],
   "source": [
    "#Prepare data\n",
    "input_train = convert_lines(comments_train.fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "toxicity_labels_train = torch.tensor(toxicity_labels_train)\n",
    "female_labels_train = torch.tensor(identity_labels_train)\n",
    "\n",
    "input_val = convert_lines(comments_val.fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "toxicity_labels_val = torch.tensor(toxicity_labels_val)\n",
    "female_labels_val = torch.tensor(identity_labels_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "colab_type": "code",
    "id": "G20XIBVr-FkQ",
    "outputId": "8fc14046-a61d-4f80-f110-7552b344c22b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5424)\n",
      "tensor(5414)\n",
      "tensor(338)\n",
      "tensor(1144)\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(toxicity_labels_train).data)\n",
    "print(torch.sum(female_labels_train).data)\n",
    "\n",
    "print(torch.sum(toxicity_labels_val).data)\n",
    "print(torch.sum(female_labels_val).data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdtIA_d27sqS"
   },
   "outputs": [],
   "source": [
    "#Data Loader\n",
    "X_train = torch.utils.data.TensorDataset(torch.tensor(input_train, dtype=torch.long), toxicity_labels_train, female_labels_train)\n",
    "train_loader = torch.utils.data.DataLoader(X_train, batch_size=32, shuffle=True)\n",
    "#tk0 = tqdm(train_loader)\n",
    "\n",
    "X_val = torch.utils.data.TensorDataset(torch.tensor(input_val, dtype=torch.long), toxicity_labels_val, female_labels_val)\n",
    "val_loader = torch.utils.data.DataLoader(X_val, batch_size=32, shuffle=True)\n",
    "#vk0 = tqdm(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WxBq_i9YtpZG"
   },
   "outputs": [],
   "source": [
    "#FAIRNESS METRICS FUNCTION\n",
    "\n",
    "def get_fairness_metrics(actual_labels, y_pred, protected_labels, non_protected_labels, thres):\n",
    "\n",
    "    def get_toxicity_rates(y_pred, protected_labels, non_protected_labels, thres):\n",
    "        protected_ops = y_pred[protected_labels == 1]\n",
    "        protected_prob = sum(protected_ops)/len(protected_ops)\n",
    "    \n",
    "        non_protected_ops = y_pred[non_protected_labels == 1]\n",
    "        non_protected_prob = sum(non_protected_ops)/len(non_protected_ops)\n",
    "    \n",
    "        return np.round(protected_prob, 2), np.round(non_protected_prob, 2)\n",
    "    \n",
    "    def get_true_positive_rates(actual_labels, y_pred, protected_labels, non_protected_labels, thres):\n",
    "\n",
    "        protected_ops = y_pred[np.bitwise_and(protected_labels == 1, actual_labels == 1)]\n",
    "        protected_prob = sum(protected_ops)/len(protected_ops)\n",
    "    \n",
    "        non_protected_ops = y_pred[np.bitwise_and(non_protected_labels == 1, actual_labels == 1)]\n",
    "        non_protected_prob = sum(non_protected_ops)/len(non_protected_ops)\n",
    "    \n",
    "        return np.round(protected_prob, 2), np.round(non_protected_prob, 2)\n",
    "    \n",
    "\n",
    "    def get_false_positive_rates(actual_labels, y_pred, protected_labels, non_protected_labels, thres):\n",
    "\n",
    "        protected_ops = y_pred[np.bitwise_and(protected_labels == 1, actual_labels ==0)]\n",
    "        protected_prob = sum(protected_ops)/len(protected_ops)\n",
    "    \n",
    "        non_protected_ops = y_pred[np.bitwise_and(non_protected_labels == 1, actual_labels == 0)]\n",
    "        non_protected_prob = sum(non_protected_ops)/len(non_protected_ops)\n",
    "    \n",
    "        return np.round(protected_prob, 2), np.round(non_protected_prob, 2)\n",
    "    \n",
    "    def demographic_parity(y_pred, protected_labels, non_protected_labels, thres):\n",
    "\n",
    "        protected_ops = y_pred[protected_labels == 1]\n",
    "        protected_prob = sum(protected_ops)/len(protected_ops)\n",
    "    \n",
    "        non_protected_ops = y_pred[non_protected_labels == 1]\n",
    "        non_protected_prob = sum(non_protected_ops)/len(non_protected_ops)\n",
    "    \n",
    "        return abs(protected_prob - non_protected_prob) #later take absolute diff - but we want to show females predicted more toxic than male\n",
    "    \n",
    "  # | P_female(C = 1| Y = 1) - P_male(C = 1 | Y = 1) | < thres\n",
    "    def true_positive_parity(actual_labels, y_pred, protected_labels, non_protected_labels, thres):\n",
    "\n",
    "        protected_ops = y_pred[np.bitwise_and(protected_labels == 1, actual_labels == 1)]\n",
    "        protected_prob = sum(protected_ops)/len(protected_ops)\n",
    "    \n",
    "        non_protected_ops = y_pred[np.bitwise_and(non_protected_labels == 1, actual_labels == 1)]\n",
    "        non_protected_prob = sum(non_protected_ops)/len(non_protected_ops)\n",
    "    \n",
    "        return abs(protected_prob - non_protected_prob) #later take absolute diff - but we want to show females predicted more toxic than male\n",
    "\n",
    "  # | P_female(C = 1| Y = 0) - P_male(C = 1 | Y = 0) | < thres\n",
    "    def false_positive_parity(actual_labels, y_pred, protected_labels, non_protected_labels, thres):\n",
    "\n",
    "        protected_ops = y_pred[np.bitwise_and(protected_labels == 1, actual_labels ==0)]\n",
    "        protected_prob = sum(protected_ops)/len(protected_ops)\n",
    "    \n",
    "        non_protected_ops = y_pred[np.bitwise_and(non_protected_labels == 1, actual_labels == 0)]\n",
    "        non_protected_prob = sum(non_protected_ops)/len(non_protected_ops)\n",
    "    \n",
    "        return abs(protected_prob - non_protected_prob) #later take absolute diff - but we want to show females predicted more toxic than male\n",
    "    \n",
    "\n",
    "  # Satisfy both true positive parity and false positive parity\n",
    "    def equalized_odds(actual_labels, y_pred, protected_labels, non_protected_labels, thres):\n",
    "        return true_positive_parity(actual_labels, y_pred, protected_labels, non_protected_labels, thres) + false_positive_parity(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
    "\n",
    "    female_tox_rate, nf_tox_rate = get_toxicity_rates(y_pred, protected_labels, non_protected_labels, thres)\n",
    "    female_tp_rate, nf_tp_rate = get_true_positive_rates(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
    "    female_fp_rate, nf_fp_rate = get_false_positive_rates(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
    "    demo_parity = demographic_parity(y_pred, protected_labels, non_protected_labels, thres)\n",
    "    tp_parity = true_positive_parity(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
    "    fp_parity = false_positive_parity(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
    "    equ_odds = equalized_odds(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
    "    return female_tox_rate, nf_tox_rate, female_tp_rate, nf_tp_rate, female_fp_rate, nf_fp_rate, demo_parity, tp_parity, fp_parity, equ_odds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TbR9gfSczmnc"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FairClassifier - BERT + equipped with adversarial network - PyTorch Implementation\n",
    "\"\"\"\n",
    "\n",
    "#set up bert with size and no hidden layers\n",
    "config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072, \n",
    "        hidden_dropout_prob=0.1)\n",
    "\n",
    "\n",
    "# classifier, take in input and classify where input is toxic?\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, toxicity_labels = 2):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        #2 layer with bert and dropout\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.c1 = nn.Linear(config.hidden_size, 324)\n",
    "        #self.c2 = nn.Linear(config.intermediate_size, 324)\n",
    "        self.c3 = nn.Linear(324, toxicity_labels)\n",
    "\n",
    "        nn.init.xavier_normal_(self.c1.weight)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        #forward call of the classifier\n",
    "        \n",
    "        #BERT leading to output and go thru dropout. Then classify\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        # Classifier classify with relu, then final is a linear layer, output toxicity \n",
    "        classifier_prev_output = F.relu(self.c1(pooled_output))\n",
    "        #classifier_prev_output = F.relu(self.c2(classifier))\n",
    "        classifier_output = self.c3(classifier_prev_output)\n",
    "\n",
    "        return classifier_output, classifier_prev_output\n",
    "\n",
    "    #adversary can be understood as a 2 layer mlp, input data and output the identity labels, which is the group information of data point x\n",
    "class Adversary(nn.Module):\n",
    "    def __init__(self, identity_labels = 2):\n",
    "        super(Adversary, self).__init__()\n",
    "        # 2 linear layers\n",
    "        self.a1 = nn.Linear(324,120)\n",
    "        self.a2 = nn.Linear(120, identity_labels)\n",
    "\n",
    "        nn.init.xavier_normal_(self.a1.weight)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "          \n",
    " \n",
    "        #Adversary relu then output identity\n",
    "        adversary = F.relu(self.a1(input_ids))\n",
    "        adversary_output = self.a2(adversary)\n",
    "\n",
    "        return adversary_output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzpkhhwCeZso"
   },
   "outputs": [],
   "source": [
    "def conduct_validation(net, data_loader, adv = False):\n",
    "\n",
    "    eval_loss, eval_accuracy, eval_precision, eval_recall, eval_f1 = 0, 0, 0, 0, 0\n",
    "    nb_eval_steps = 0\n",
    "    \n",
    "    predictions_net = np.empty((0,))\n",
    "    truths = np.empty((0,))\n",
    "    identities = np.empty((0,))\n",
    "    correct_net = 0\n",
    "    total = 0\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad(): # IMPORTANT: we don't want to do back prop during validation/testing!\n",
    "        for index, data in enumerate(data_loader):\n",
    "\n",
    "            text, toxic_truth, female_truth = data\n",
    "    \n",
    "            text = text.to(device)\n",
    "            toxic_truth = toxic_truth.to(device)\n",
    "            female_truth = female_truth.to(device)\n",
    "    \n",
    "            if adv:\n",
    "                net_outputs, net_prev_outputs = net(text)\n",
    "            else:\n",
    "                net_outputs = net(text)\n",
    "                _, net_predicted = torch.max(net_outputs.data, 1)\n",
    "    \n",
    "            batch_size = toxic_truth.size(0)\n",
    "            total += batch_size\n",
    "            correct_net_batch = (net_predicted == toxic_truth).sum().item()\n",
    "            correct_net += correct_net_batch\n",
    "    \n",
    "            \n",
    "            predictions_net = np.concatenate((predictions_net, net_predicted.cpu().numpy()))\n",
    "            truths = np.concatenate((truths, toxic_truth.cpu().numpy()))\n",
    "            identities = np.concatenate((identities, female_truth.cpu().numpy()))\n",
    "    \n",
    "            pred = net_predicted.detach().cpu().numpy()\n",
    "            label_ids = toxic_truth.to('cpu').numpy()\n",
    "    \n",
    "            tmp_eval_accuracy, tmp_eval_precision, temp_eval_recall, tmp_eval_f1 = get_metrics(label_ids, pred)\n",
    "    \n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            eval_precision += tmp_eval_precision\n",
    "            eval_recall += temp_eval_recall\n",
    "            eval_f1 += tmp_eval_f1\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "    f1_score = eval_f1/nb_eval_steps\n",
    "    prec_score = eval_precision/nb_eval_steps\n",
    "    recall_score = eval_recall/nb_eval_steps\n",
    "    acc_score = eval_accuracy/nb_eval_steps\n",
    "\n",
    "    print(\"F1 Score: \", f1_score)\n",
    "    print(\"Precision Score: \", prec_score)\n",
    "    print(\"Recall Score: \", recall_score)\n",
    "    print(\"Acc Score: \", acc_score, \"\\n\\n\")\n",
    "\n",
    "    net.train()\n",
    "    \n",
    "    return (predictions_net, truths, identities, acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fwF1aQ_EzOI"
   },
   "outputs": [],
   "source": [
    "def pretrain_classifier(clf, optimizer_clf, train_loader, loss_criterion, epochs):\n",
    "    #pre train the clf before introducing the adversary?\n",
    "    pretrain_classifier_loss = 0\n",
    "    steps = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(\"Epoch: \", epoch + 1)\n",
    "        epoch_loss = 0\n",
    "        epoch_batches = 0\n",
    "    \n",
    "        for i, data in enumerate(train_loader): # starting from the 0th batch\n",
    "            # get the inputs and labels\n",
    "            inputs, toxicity_true, female_true = data\n",
    "            inputs = inputs.to(device)\n",
    "            # toxicity_true = torch.sparse.torch.eye(2).index_select(dim=0, index=toxicity_true) \n",
    "            # female_true = torch.sparse.torch.eye(2).index_select(dim=0, index=female_true) \n",
    "            toxicity_true = toxicity_true.to(device)\n",
    "            female_true = female_true.to(device)\n",
    "            \n",
    "            #optimize for classification, zeroing grad in each iter to accumulately update the \n",
    "            optimizer_clf.zero_grad()\n",
    "            \n",
    "            #get inference, prediction from the clf\n",
    "            classifier_output, _ = clf(inputs)\n",
    "            #compute the classifier loss @ this epoch from the true label and predicted values\n",
    "            classifier_loss = loss_criterion(classifier_output, toxicity_true) # compute loss\n",
    "            classifier_loss.backward() # back prop\n",
    "            optimizer_clf.step()#next step = update\n",
    "            \n",
    "            #prepare output for printout to keep track of the training process\n",
    "            pretrain_classifier_loss += classifier_loss.item()\n",
    "            epoch_loss += classifier_loss.item()\n",
    "            epoch_batches += 1\n",
    "            steps += 1\n",
    "    \n",
    "            print(\"Average Pretrain Classifier epoch loss: \", epoch_loss/epoch_batches)\n",
    "            print(\"Average Pretrain Classifier batch loss: \", pretrain_classifier_loss/steps)\n",
    "\n",
    "    return clf\n",
    "\n",
    "#similarly, pretrain the adversary\n",
    "def pretrain_adversary(adv, clf, optimizer_adv, train_loader, loss_criterion, epochs):\n",
    "  \n",
    "    pretrain_adversary_loss = 0\n",
    "    steps = 0\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(\"Epoch: \", epoch + 1)\n",
    "        epoch_loss = 0\n",
    "        epoch_batches = 0\n",
    "        for i, data in enumerate(train_loader): # starting from the 0th batch\n",
    "            # get the inputs and labels\n",
    "            inputs, toxicity_true, female_true = data\n",
    "            inputs = inputs.to(device)\n",
    "            # toxicity_true = torch.sparse.torch.eye(2).index_select(dim=0, index=toxicity_true) \n",
    "            # female_true = torch.sparse.torch.eye(2).index_select(dim=0, index=female_true) \n",
    "            toxicity_true = toxicity_true.to(device)\n",
    "            female_true = female_true.to(device)\n",
    "    \n",
    "            optimizer_adv.zero_grad()\n",
    "    \n",
    "            _, classifier_prev_output = clf(inputs)\n",
    "            adversary_output = adv(classifier_prev_output)\n",
    "            adversary_loss = loss_criterion(adversary_output, female_true) # compute loss\n",
    "            adversary_loss.backward() # back prop\n",
    "            optimizer_adv.step()\n",
    "            \n",
    "            #this is for the printout, keeping track\n",
    "            pretrain_adversary_loss += adversary_loss.item()\n",
    "            epoch_loss += adversary_loss.item()\n",
    "            epoch_batches += 1\n",
    "            steps += 1\n",
    "    \n",
    "            print(\"Average Pretrain Adversary epoch loss: \", epoch_loss/epoch_batches)\n",
    "            print(\"Average Pretrain Adversary batch loss: \", pretrain_adversary_loss/steps)\n",
    "\n",
    "    return adv\n",
    "\n",
    "\n",
    "def train_adversary(adv, clf, optimizer_adv, train_loader, loss_criterion, epochs=1):\n",
    "  \n",
    "    adv_loss = 0\n",
    "    steps = 0\n",
    "    for epoch in range(epochs):\n",
    "        for i, data in enumerate(train_loader): # starting from the 0th batch\n",
    "            # get the inputs and labels\n",
    "            inputs, toxicity_true, female_true = data\n",
    "            inputs = inputs.to(device)\n",
    "            # toxicity_true = torch.sparse.torch.eye(2).index_select(dim=0, index=toxicity_true) \n",
    "            # female_true = torch.sparse.torch.eye(2).index_select(dim=0, index=female_true) \n",
    "            toxicity_true = toxicity_true.to(device)\n",
    "            female_true = female_true.to(device)\n",
    "    \n",
    "            optimizer_adv.zero_grad()\n",
    "    \n",
    "            classifier_output, classifier_prev_output = clf(inputs)\n",
    "            adversary_output = adv(classifier_prev_output)\n",
    "            adversary_loss = loss_criterion(adversary_output, female_true) # compute loss\n",
    "            adversary_loss.backward() # back prop\n",
    "            optimizer_adv.step()\n",
    "            adv_loss += adversary_loss.item()\n",
    "            steps += 1\n",
    "      \n",
    "            print(\"Average Adversary batch loss: \", adv_loss/steps)\n",
    "    return adv\n",
    "\n",
    "def train_classifier(clf, optimizer_clf, adv, train_loader, loss_criterion, lbda):\n",
    "\n",
    "    for i, data in enumerate(train_loader): # starting from the 0th batch\n",
    "        # get the inputs and labels\n",
    "        inputs, toxicity_true, female_true = data\n",
    "        inputs = inputs.to(device)\n",
    "        # toxicity_true = torch.sparse.torch.eye(2).index_select(dim=0, index=toxicity_true) \n",
    "        # female_true = torch.sparse.torch.eye(2).index_select(dim=0, index=female_true) \n",
    "        toxicity_true = toxicity_true.to(device)\n",
    "        female_true = female_true.to(device)\n",
    "        # Toxic classifier part\n",
    "        optimizer_clf.zero_grad()\n",
    "        classifier_output, classifier_prev_output = clf(inputs)\n",
    "        adversary_output = adv(classifier_prev_output)\n",
    "        adversary_loss = loss_criterion(adversary_output, female_true)\n",
    "        classifier_loss = loss_criterion(classifier_output, toxicity_true) # compute loss\n",
    "        total_classifier_loss = classifier_loss - lbda * adversary_loss\n",
    "        total_classifier_loss.backward() # back prop\n",
    "        \n",
    "        optimizer_clf.step()\n",
    "        print(\"Adversary Mini-Batch loss: \", adversary_loss.item())\n",
    "        print(\"Classifier Mini-Batch loss: \", classifier_loss.item())\n",
    "        print(\"Total Mini-Batch loss: \", total_classifier_loss.item())\n",
    "        break\n",
    "\n",
    "    return clf\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5K3q0aEn4SpJ",
    "outputId": "6a146b8d-f862-4dc4-b9bd-3157b50c525d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/407873900 [00:00<?, ?B/s]\u001b[A\n",
      "  0%|          | 17408/407873900 [00:00<1:09:14, 98169.23B/s]\u001b[A\n",
      "  0%|          | 52224/407873900 [00:00<59:01, 115156.92B/s] \u001b[A\n",
      "  0%|          | 121856/407873900 [00:00<46:35, 145866.68B/s]\u001b[A\n",
      "  0%|          | 243712/407873900 [00:00<35:37, 190730.85B/s]\u001b[A\n",
      "  0%|          | 348160/407873900 [00:00<28:26, 238758.98B/s]\u001b[A\n",
      "  0%|          | 696320/407873900 [00:01<20:57, 323924.85B/s]\u001b[A\n",
      "  0%|          | 1114112/407873900 [00:01<15:31, 436578.18B/s]\u001b[A\n",
      "  0%|          | 1513472/407873900 [00:01<11:46, 575083.33B/s]\u001b[A\n",
      "  1%|          | 2865152/407873900 [00:01<08:29, 795373.65B/s]\u001b[A\n",
      "  1%|          | 3962880/407873900 [00:01<06:15, 1075978.48B/s]\u001b[A\n",
      "  1%|▏         | 5322752/407873900 [00:01<04:37, 1448495.99B/s]\u001b[A\n",
      "  2%|▏         | 7731200/407873900 [00:02<03:22, 1977353.74B/s]\u001b[A\n",
      "  2%|▏         | 9517056/407873900 [00:02<02:33, 2601930.89B/s]\u001b[A\n",
      "  3%|▎         | 11712512/407873900 [00:02<01:56, 3404687.33B/s]\u001b[A\n",
      "  3%|▎         | 13711360/407873900 [00:02<01:31, 4297827.97B/s]\u001b[A\n",
      "  4%|▍         | 15824896/407873900 [00:02<01:13, 5305693.95B/s]\u001b[A\n",
      "  4%|▍         | 17790976/407873900 [00:03<01:02, 6270398.86B/s]\u001b[A\n",
      "  5%|▍         | 19822592/407873900 [00:03<00:53, 7231879.99B/s]\u001b[A\n",
      "  5%|▌         | 21919744/407873900 [00:03<00:47, 8156501.18B/s]\u001b[A\n",
      "  6%|▌         | 23902208/407873900 [00:03<00:43, 8839302.36B/s]\u001b[A\n",
      "  6%|▋         | 25933824/407873900 [00:03<00:40, 9449263.08B/s]\u001b[A\n",
      "  7%|▋         | 27932672/407873900 [00:03<00:38, 9888071.31B/s]\u001b[A\n",
      "  7%|▋         | 29964288/407873900 [00:04<00:36, 10261793.78B/s]\u001b[A\n",
      "  8%|▊         | 31979520/407873900 [00:04<00:35, 10519770.65B/s]\u001b[A\n",
      "  8%|▊         | 34011136/407873900 [00:04<00:34, 10732508.18B/s]\u001b[A\n",
      "  9%|▉         | 36141056/407873900 [00:04<00:33, 11034150.08B/s]\u001b[A\n",
      "  9%|▉         | 38172672/407873900 [00:04<00:33, 11101532.25B/s]\u001b[A\n",
      " 10%|▉         | 40220672/407873900 [00:05<00:32, 11177642.43B/s]\u001b[A\n",
      " 10%|█         | 42317824/407873900 [00:05<00:32, 11309566.94B/s]\u001b[A\n",
      " 11%|█         | 44382208/407873900 [00:05<00:32, 11349996.12B/s]\u001b[A\n",
      " 11%|█▏        | 46512128/407873900 [00:05<00:31, 11483565.41B/s]\u001b[A\n",
      " 12%|█▏        | 48543744/407873900 [00:05<00:31, 11414939.08B/s]\u001b[A\n",
      " 12%|█▏        | 50640896/407873900 [00:05<00:31, 11481763.43B/s]\u001b[A\n",
      " 13%|█▎        | 52705280/407873900 [00:06<00:30, 11472714.47B/s]\u001b[A\n",
      " 13%|█▎        | 54769664/407873900 [00:06<00:30, 11463155.28B/s]\u001b[A\n",
      " 14%|█▍        | 56834048/407873900 [00:06<00:30, 11458583.75B/s]\u001b[A\n",
      " 14%|█▍        | 58996736/407873900 [00:06<00:30, 11610778.14B/s]\u001b[A\n",
      " 15%|█▍        | 61011968/407873900 [00:06<00:30, 11473019.12B/s]\u001b[A\n",
      " 15%|█▌        | 63109120/407873900 [00:07<00:29, 11516638.90B/s]\u001b[A\n",
      " 16%|█▌        | 65189888/407873900 [00:07<00:29, 11522078.81B/s]\u001b[A\n",
      " 16%|█▋        | 67237888/407873900 [00:07<00:29, 11471818.85B/s]\u001b[A\n",
      " 17%|█▋        | 69367808/407873900 [00:07<00:29, 11571576.88B/s]\u001b[A\n",
      " 17%|█▋        | 71366656/407873900 [00:07<00:29, 11421355.17B/s]\u001b[A\n",
      " 18%|█▊        | 73381888/407873900 [00:07<00:29, 11344714.08B/s]\u001b[A\n",
      " 19%|█▊        | 75741184/407873900 [00:08<00:28, 11813646.14B/s]\u001b[A\n",
      " 19%|█▉        | 77756416/407873900 [00:08<00:28, 11614614.75B/s]\u001b[A\n",
      " 20%|█▉        | 80082944/407873900 [00:08<00:27, 11973130.60B/s]\u001b[A\n",
      " 20%|██        | 82081792/407873900 [00:08<00:27, 11688896.92B/s]\u001b[A\n",
      " 21%|██        | 84342784/407873900 [00:08<00:27, 11931052.60B/s]\u001b[A\n",
      " 21%|██        | 86423552/407873900 [00:09<00:27, 11809829.05B/s]\u001b[A\n",
      " 22%|██▏       | 88569856/407873900 [00:09<00:26, 11840196.03B/s]\u001b[A\n",
      " 22%|██▏       | 90716160/407873900 [00:09<00:26, 11857730.63B/s]\u001b[A\n",
      " 23%|██▎       | 92911616/407873900 [00:09<00:26, 11952397.41B/s]\u001b[A\n",
      " 23%|██▎       | 95123456/407873900 [00:09<00:25, 12042261.84B/s]\u001b[A\n",
      " 24%|██▍       | 97007616/407873900 [00:09<00:27, 11511710.27B/s]\u001b[A\n",
      " 24%|██▍       | 99203072/407873900 [00:10<00:26, 11701285.34B/s]\u001b[A\n",
      " 25%|██▍       | 101316608/407873900 [00:10<00:26, 11707521.11B/s]\u001b[A\n",
      " 25%|██▌       | 103380992/407873900 [00:10<00:26, 11625259.44B/s]\u001b[A\n",
      " 26%|██▌       | 105428992/407873900 [00:10<00:26, 11539947.60B/s]\u001b[A\n",
      " 26%|██▋       | 107558912/407873900 [00:10<00:25, 11621221.73B/s]\u001b[A\n",
      " 27%|██▋       | 109803520/407873900 [00:11<00:25, 11854849.39B/s]\u001b[A\n",
      " 27%|██▋       | 111867904/407873900 [00:11<00:25, 11729410.97B/s]\u001b[A\n",
      " 28%|██▊       | 114063360/407873900 [00:11<00:24, 11858120.22B/s]\u001b[A\n",
      " 28%|██▊       | 116078592/407873900 [00:11<00:25, 11642881.11B/s]\u001b[A\n",
      " 29%|██▉       | 118208512/407873900 [00:11<00:24, 11691636.31B/s]\u001b[A\n",
      " 29%|██▉       | 120256512/407873900 [00:11<00:24, 11589338.14B/s]\u001b[A\n",
      " 30%|██▉       | 122353664/407873900 [00:12<00:24, 11599991.51B/s]\u001b[A\n",
      " 31%|███       | 124467200/407873900 [00:12<00:24, 11637881.36B/s]\u001b[A\n",
      " 31%|███       | 126613504/407873900 [00:12<00:24, 11716283.16B/s]\u001b[A\n",
      " 32%|███▏      | 128792576/407873900 [00:12<00:23, 11823701.03B/s]\u001b[A\n",
      " 32%|███▏      | 130906112/407873900 [00:12<00:23, 11791331.87B/s]\u001b[A\n",
      " 33%|███▎      | 133068800/407873900 [00:12<00:23, 11850333.70B/s]\u001b[A\n",
      " 33%|███▎      | 135215104/407873900 [00:13<00:22, 11863650.05B/s]\u001b[A\n",
      " 34%|███▎      | 137345024/407873900 [00:13<00:22, 11850841.76B/s]\u001b[A\n",
      " 34%|███▍      | 139442176/407873900 [00:13<00:22, 11783963.92B/s]\u001b[A\n",
      " 35%|███▍      | 141768704/407873900 [00:13<00:21, 12098993.08B/s]\u001b[A\n",
      " 35%|███▌      | 143964160/407873900 [00:13<00:21, 12122464.01B/s]\u001b[A\n",
      " 36%|███▌      | 146208768/407873900 [00:14<00:21, 12214669.18B/s]\u001b[A\n",
      " 36%|███▋      | 148715520/407873900 [00:14<00:20, 12673952.19B/s]\u001b[A\n",
      " 37%|███▋      | 150943744/407873900 [00:14<00:20, 12577701.50B/s]\u001b[A\n",
      " 38%|███▊      | 153139200/407873900 [00:14<00:20, 12455498.86B/s]\u001b[A\n",
      " 38%|███▊      | 155596800/407873900 [00:14<00:19, 12787588.55B/s]\u001b[A\n",
      " 39%|███▊      | 157857792/407873900 [00:14<00:19, 12712511.53B/s]\u001b[A\n",
      " 39%|███▉      | 160135168/407873900 [00:15<00:19, 12688778.26B/s]\u001b[A\n",
      " 40%|███▉      | 162379776/407873900 [00:15<00:19, 12615094.28B/s]\u001b[A\n",
      " 40%|████      | 164739072/407873900 [00:15<00:19, 12748996.96B/s]\u001b[A\n",
      " 41%|████      | 166852608/407873900 [00:15<00:19, 12417966.85B/s]\u001b[A\n",
      " 41%|████▏     | 169146368/407873900 [00:15<00:19, 12503502.31B/s]\u001b[A\n",
      " 42%|████▏     | 171407360/407873900 [00:16<00:18, 12513651.73B/s]\u001b[A\n",
      " 43%|████▎     | 173684736/407873900 [00:16<00:18, 12546036.83B/s]\u001b[A\n",
      " 43%|████▎     | 175831040/407873900 [00:16<00:18, 12342433.26B/s]\u001b[A\n",
      " 44%|████▎     | 178075648/407873900 [00:16<00:18, 12372425.50B/s]\u001b[A\n",
      " 44%|████▍     | 180402176/407873900 [00:16<00:18, 12522980.48B/s]\u001b[A\n",
      " 45%|████▍     | 182564864/407873900 [00:16<00:18, 12357245.63B/s]\u001b[A\n",
      " 45%|████▌     | 184858624/407873900 [00:17<00:17, 12462759.10B/s]\u001b[A\n",
      " 46%|████▌     | 187037696/407873900 [00:17<00:17, 12346658.73B/s]\u001b[A\n",
      " 46%|████▋     | 189265920/407873900 [00:17<00:17, 12348800.83B/s]\u001b[A\n",
      " 47%|████▋     | 191625216/407873900 [00:17<00:17, 12556416.86B/s]\u001b[A\n",
      " 47%|████▋     | 193673216/407873900 [00:17<00:17, 12173570.80B/s]\u001b[A\n",
      " 48%|████▊     | 195852288/407873900 [00:18<00:17, 12145536.00B/s]\u001b[A\n",
      " 49%|████▊     | 198244352/407873900 [00:18<00:16, 12459518.87B/s]\u001b[A\n",
      " 49%|████▉     | 200210432/407873900 [00:18<00:17, 11946442.37B/s]\u001b[A\n",
      " 50%|████▉     | 202487808/407873900 [00:18<00:16, 12139432.12B/s]\u001b[A\n",
      " 50%|█████     | 204716032/407873900 [00:18<00:16, 12202685.36B/s]\u001b[A\n",
      " 51%|█████     | 206895104/407873900 [00:18<00:16, 12164039.69B/s]\u001b[A\n",
      " 51%|█████▏    | 209221632/407873900 [00:19<00:16, 12375583.57B/s]\u001b[A\n",
      " 52%|█████▏    | 211548160/407873900 [00:19<00:15, 12526118.12B/s]\u001b[A\n",
      " 52%|█████▏    | 213645312/407873900 [00:19<00:15, 12244714.98B/s]\u001b[A\n",
      " 53%|█████▎    | 215873536/407873900 [00:19<00:15, 12275569.22B/s]\u001b[A\n",
      " 54%|█████▎    | 218314752/407873900 [00:19<00:15, 12629750.63B/s]\u001b[A\n",
      " 54%|█████▍    | 220477440/407873900 [00:20<00:15, 12431581.07B/s]\u001b[A\n",
      " 55%|█████▍    | 222672896/407873900 [00:20<00:14, 12352074.31B/s]\u001b[A\n",
      " 55%|█████▌    | 224868352/407873900 [00:20<00:14, 12298918.96B/s]\u001b[A\n",
      " 56%|█████▌    | 227244032/407873900 [00:20<00:14, 12550106.68B/s]\u001b[A\n",
      " 56%|█████▌    | 229373952/407873900 [00:20<00:14, 12319686.24B/s]\u001b[A\n",
      " 57%|█████▋    | 231766016/407873900 [00:20<00:13, 12590751.31B/s]\u001b[A\n",
      " 57%|█████▋    | 233830400/407873900 [00:21<00:14, 12225700.34B/s]\u001b[A\n",
      " 58%|█████▊    | 236189696/407873900 [00:21<00:13, 12469919.36B/s]\u001b[A\n",
      " 58%|█████▊    | 238434304/407873900 [00:21<00:13, 12463576.62B/s]\u001b[A\n",
      " 59%|█████▉    | 240777216/407873900 [00:21<00:13, 12614569.00B/s]\u001b[A\n",
      " 60%|█████▉    | 243070976/407873900 [00:21<00:13, 12645338.37B/s]\u001b[A\n",
      " 60%|██████    | 245331968/407873900 [00:22<00:12, 12613576.90B/s]\u001b[A\n",
      " 61%|██████    | 247379968/407873900 [00:22<00:13, 12206117.43B/s]\u001b[A\n",
      " 61%|██████    | 249804800/407873900 [00:22<00:12, 12554164.11B/s]\u001b[A\n",
      " 62%|██████▏   | 252065792/407873900 [00:22<00:12, 12551690.73B/s]\u001b[A\n",
      " 63%|██████▎   | 254965760/407873900 [00:22<00:11, 13435640.28B/s]\u001b[A\n",
      " 63%|██████▎   | 257521664/407873900 [00:22<00:11, 13647385.14B/s]\u001b[A\n",
      " 64%|██████▍   | 260159488/407873900 [00:23<00:10, 13927904.55B/s]\u001b[A\n",
      " 64%|██████▍   | 262830080/407873900 [00:23<00:10, 14181444.03B/s]\u001b[A\n",
      " 65%|██████▌   | 265435136/407873900 [00:23<00:09, 14258729.31B/s]\u001b[A\n",
      " 66%|██████▌   | 268040192/407873900 [00:23<00:09, 14317749.15B/s]\u001b[A\n",
      " 66%|██████▋   | 270645248/407873900 [00:23<00:09, 14353820.01B/s]\u001b[A\n",
      " 67%|██████▋   | 273250304/407873900 [00:23<00:09, 14380989.10B/s]\u001b[A\n",
      " 68%|██████▊   | 275773440/407873900 [00:24<00:09, 14263080.70B/s]\u001b[A\n",
      " 68%|██████▊   | 278411264/407873900 [00:24<00:09, 14370924.00B/s]\u001b[A\n",
      " 69%|██████▉   | 280934400/407873900 [00:24<00:08, 14254264.99B/s]\u001b[A\n",
      " 70%|██████▉   | 283588608/407873900 [00:24<00:08, 14390904.43B/s]\u001b[A\n",
      " 70%|███████   | 286259200/407873900 [00:24<00:08, 14513843.96B/s]\u001b[A\n",
      " 71%|███████   | 288684032/407873900 [00:25<00:08, 14175050.86B/s]\u001b[A\n",
      " 71%|███████▏  | 291485696/407873900 [00:25<00:07, 14553671.48B/s]\u001b[A\n",
      " 72%|███████▏  | 294172672/407873900 [00:25<00:07, 14657774.49B/s]\u001b[A\n",
      " 73%|███████▎  | 296908800/407873900 [00:25<00:07, 14807704.90B/s]\u001b[A\n",
      " 73%|███████▎  | 299464704/407873900 [00:25<00:07, 14612052.18B/s]\u001b[A\n",
      " 74%|███████▍  | 302135296/407873900 [00:25<00:07, 14666734.28B/s]\u001b[A\n",
      " 75%|███████▍  | 304822272/407873900 [00:26<00:06, 14733709.42B/s]\u001b[A\n",
      " 75%|███████▌  | 307525632/407873900 [00:26<00:06, 14809053.80B/s]\u001b[A\n",
      " 76%|███████▌  | 310048768/407873900 [00:26<00:06, 14550905.64B/s]\u001b[A\n",
      " 77%|███████▋  | 312752128/407873900 [00:26<00:06, 14680864.17B/s]\u001b[A\n",
      " 77%|███████▋  | 315471872/407873900 [00:26<00:06, 14802125.09B/s]\u001b[A\n",
      " 78%|███████▊  | 318126080/407873900 [00:27<00:06, 14776989.71B/s]\u001b[A\n",
      " 79%|███████▊  | 320763904/407873900 [00:27<00:05, 14727083.10B/s]\u001b[A\n",
      " 79%|███████▉  | 323057664/407873900 [00:27<00:06, 14057571.08B/s]\u001b[A\n",
      " 80%|███████▉  | 325908480/407873900 [00:27<00:05, 14541043.96B/s]\u001b[A\n",
      " 81%|████████  | 328431616/407873900 [00:27<00:05, 14369449.71B/s]\u001b[A\n",
      " 81%|████████  | 331216896/407873900 [00:27<00:05, 14675416.77B/s]\u001b[A\n",
      " 82%|████████▏ | 333821952/407873900 [00:28<00:05, 14598421.04B/s]\u001b[A\n",
      " 83%|████████▎ | 336508928/407873900 [00:28<00:04, 14688773.45B/s]\u001b[A\n",
      " 83%|████████▎ | 339163136/407873900 [00:28<00:04, 14695011.91B/s]\u001b[A\n",
      " 84%|████████▍ | 341784576/407873900 [00:28<00:04, 14647265.65B/s]\u001b[A\n",
      " 84%|████████▍ | 343635968/407873900 [00:28<00:04, 12984972.38B/s]\u001b[A\n",
      " 85%|████████▍ | 346470400/407873900 [00:29<00:04, 13697785.19B/s]\u001b[A\n",
      " 86%|████████▌ | 349190144/407873900 [00:29<00:04, 14081081.42B/s]\u001b[A\n",
      " 86%|████████▋ | 351926272/407873900 [00:29<00:03, 14393564.98B/s]\u001b[A\n",
      " 87%|████████▋ | 354662400/407873900 [00:29<00:03, 14617607.43B/s]\u001b[A\n",
      " 88%|████████▊ | 357480448/407873900 [00:29<00:03, 14903058.13B/s]\u001b[A\n",
      " 88%|████████▊ | 360085504/407873900 [00:29<00:03, 14759572.63B/s]\u001b[A\n",
      " 89%|████████▉ | 362838016/407873900 [00:30<00:03, 14906351.40B/s]\u001b[A\n",
      " 90%|████████▉ | 365623296/407873900 [00:30<00:02, 15059575.05B/s]\u001b[A\n",
      " 90%|█████████ | 368375808/407873900 [00:30<00:02, 15119567.51B/s]\u001b[A\n",
      " 91%|█████████ | 371144704/407873900 [00:30<00:02, 15190910.34B/s]\u001b[A\n",
      " 92%|█████████▏| 373979136/407873900 [00:30<00:02, 15330667.17B/s]\u001b[A\n",
      " 92%|█████████▏| 376485888/407873900 [00:31<00:02, 14866364.53B/s]\u001b[A\n",
      " 93%|█████████▎| 379303936/407873900 [00:31<00:01, 15084292.13B/s]\u001b[A\n",
      " 94%|█████████▎| 381581312/407873900 [00:31<00:01, 14252806.73B/s]\u001b[A\n",
      " 94%|█████████▍| 384661504/407873900 [00:31<00:01, 14994521.01B/s]\u001b[A\n",
      " 95%|█████████▍| 387299328/407873900 [00:31<00:01, 14883109.65B/s]\u001b[A\n",
      " 96%|█████████▌| 390100992/407873900 [00:31<00:01, 15069564.92B/s]\u001b[A\n",
      " 96%|█████████▋| 392771584/407873900 [00:32<00:01, 14986556.38B/s]\u001b[A\n",
      " 97%|█████████▋| 395507712/407873900 [00:32<00:00, 15041371.14B/s]\u001b[A\n",
      " 98%|█████████▊| 398276608/407873900 [00:32<00:00, 15128797.46B/s]\u001b[A\n",
      " 98%|█████████▊| 401029120/407873900 [00:32<00:00, 15167631.96B/s]\u001b[A\n",
      " 99%|█████████▉| 403748864/407873900 [00:32<00:00, 15142429.98B/s]\u001b[A\n",
      "100%|█████████▉| 406550528/407873900 [00:33<00:00, 15254750.09B/s]\u001b[A\n",
      "100%|██████████| 407873900/407873900 [00:33<00:00, 12348685.71B/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "Average Pretrain Classifier epoch loss:  0.5526828840592165\n",
      "Epoch:  2\n",
      "Average Pretrain Classifier epoch loss:  0.34685237462423285\n",
      "Epoch:  3\n",
      "Average Pretrain Classifier epoch loss:  0.26637104820492663\n",
      "Average Pretrain Classifier batch loss:  0.3886354356294587\n",
      "Epoch:  1\n",
      "Average Pretrain Adversary epoch loss:  0.35933143004731605\n",
      "Epoch:  2\n",
      "Average Pretrain Adversary epoch loss:  0.23223073587212548\n",
      "Epoch:  3\n",
      "Average Pretrain Adversary epoch loss:  0.21500630925461084\n",
      "Average Pretrain Adversary batch loss:  0.2688561583913508\n",
      "Lambda: 3\n",
      "Iteration:  0\n",
      "Average Adversary batch loss:  0.19585806231283237\n",
      "Adversary Mini-Batch loss:  0.3093579411506653\n",
      "Classifier Mini-Batch loss:  0.28543177247047424\n",
      "Total Mini-Batch loss:  -0.6426420211791992\n",
      "Iteration:  1\n",
      "Average Adversary batch loss:  0.29629818367454125\n",
      "Adversary Mini-Batch loss:  0.3134836256504059\n",
      "Classifier Mini-Batch loss:  0.1830923855304718\n",
      "Total Mini-Batch loss:  -0.7573585510253906\n",
      "Training metrics:\n",
      "F1 Score:  0.9307837809502906\n",
      "Precision Score:  0.8855582544052337\n",
      "Recall Score:  0.986936452648552\n",
      "Acc Score:  0.9309402332361516 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness Metrics on Train:\n",
      "Toxicity Prediction Rates:  Female - 0.76 Non-Female -  0.34\n",
      "True Positive Prediction Rates:  Female - 0.99 Non-Female -  0.98\n",
      "False Positive Prediction Rates:  Female - 0.21 Non-Female -  0.09\n",
      "Demographic Parity:  0.416025997081906\n",
      "True Positive Parity:  0.0104418202553066\n",
      "False Positive Parity:  0.12132864338565964\n",
      "Equalized Odds:  0.13177046364096623\n",
      "\n",
      "\n",
      "Validation metrics:\n",
      "F1 Score:  0.7446261302213726\n",
      "Precision Score:  0.3542736079832495\n",
      "Recall Score:  0.9098702443940538\n",
      "Acc Score:  0.7078373015873016 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness Metrics on Validation:\n",
      "Toxicity Prediction Rates:  Female - 0.5 Non-Female -  0.32\n",
      "True Positive Prediction Rates:  Female - 0.93 Non-Female -  0.87\n",
      "False Positive Prediction Rates:  Female - 0.42 Non-Female -  0.2\n",
      "Demographic Parity:  0.17989837265538206\n",
      "True Positive Parity:  0.060448683978095774\n",
      "False Positive Parity:  0.2164357431357047\n",
      "Equalized Odds:  0.27688442711380046\n",
      "\n",
      "\n",
      "\n",
      "__________________\n",
      "Iteration:  2\n",
      "Average Adversary batch loss:  0.3936879022302155\n",
      "Adversary Mini-Batch loss:  0.33663851022720337\n",
      "Classifier Mini-Batch loss:  0.20409944653511047\n",
      "Total Mini-Batch loss:  -0.8058161735534668\n",
      "Iteration:  3\n",
      "Average Adversary batch loss:  0.44961100469177734\n",
      "Adversary Mini-Batch loss:  0.28914541006088257\n",
      "Classifier Mini-Batch loss:  0.09130791574716568\n",
      "Total Mini-Batch loss:  -0.7761282920837402\n",
      "Training metrics:\n",
      "F1 Score:  0.8751097375244788\n",
      "Precision Score:  0.8018253555143631\n",
      "Recall Score:  0.994473830233247\n",
      "Acc Score:  0.876773566569485 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness Metrics on Train:\n",
      "Toxicity Prediction Rates:  Female - 0.81 Non-Female -  0.42\n",
      "True Positive Prediction Rates:  Female - 1.0 Non-Female -  0.99\n",
      "False Positive Prediction Rates:  Female - 0.35 Non-Female -  0.19\n",
      "Demographic Parity:  0.382667371828433\n",
      "True Positive Parity:  0.004311027725661831\n",
      "False Positive Parity:  0.15500897755634746\n",
      "Equalized Odds:  0.1593200052820093\n",
      "\n",
      "\n",
      "Validation metrics:\n",
      "F1 Score:  0.6683314501721384\n",
      "Precision Score:  0.30395764638500633\n",
      "Recall Score:  0.94572940287226\n",
      "Acc Score:  0.6235119047619048 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness Metrics on Validation:\n",
      "Toxicity Prediction Rates:  Female - 0.6 Non-Female -  0.42\n",
      "True Positive Prediction Rates:  Female - 0.96 Non-Female -  0.93\n",
      "False Positive Prediction Rates:  Female - 0.54 Non-Female -  0.31\n",
      "Demographic Parity:  0.18316613293248807\n",
      "True Positive Parity:  0.027521639286345212\n",
      "False Positive Parity:  0.2258754006737103\n",
      "Equalized Odds:  0.2533970399600555\n",
      "\n",
      "\n",
      "\n",
      "__________________\n",
      "Iteration:  4\n",
      "Average Adversary batch loss:  0.38117447391195824\n",
      "Adversary Mini-Batch loss:  0.29044967889785767\n",
      "Classifier Mini-Batch loss:  0.2797451913356781\n",
      "Total Mini-Batch loss:  -0.5916038751602173\n",
      "Iteration:  5\n",
      "Average Adversary batch loss:  0.42818711298373974\n",
      "Adversary Mini-Batch loss:  0.5872876644134521\n",
      "Classifier Mini-Batch loss:  0.2829042077064514\n",
      "Total Mini-Batch loss:  -1.4789588451385498\n",
      "Training metrics:\n",
      "F1 Score:  0.8326885958703064\n",
      "Precision Score:  0.7536592046899865\n",
      "Recall Score:  0.9958063271578151\n",
      "Acc Score:  0.8371598639455782 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness Metrics on Train:\n",
      "Toxicity Prediction Rates:  Female - 0.83 Non-Female -  0.48\n",
      "True Positive Prediction Rates:  Female - 1.0 Non-Female -  0.99\n",
      "False Positive Prediction Rates:  Female - 0.43 Non-Female -  0.27\n",
      "Demographic Parity:  0.3496696547453577\n",
      "True Positive Parity:  0.0008201204183987398\n",
      "False Positive Parity:  0.15805345034158563\n",
      "Equalized Odds:  0.15887357075998437\n",
      "\n",
      "\n",
      "Validation metrics:\n",
      "F1 Score:  0.624890918348328\n",
      "Precision Score:  0.2815448765010168\n",
      "Recall Score:  0.9716824621586526\n",
      "Acc Score:  0.5768849206349206 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness Metrics on Validation:\n",
      "Toxicity Prediction Rates:  Female - 0.65 Non-Female -  0.49\n",
      "True Positive Prediction Rates:  Female - 0.97 Non-Female -  0.95\n",
      "False Positive Prediction Rates:  Female - 0.58 Non-Female -  0.39\n",
      "Demographic Parity:  0.15882948826874055\n",
      "True Positive Parity:  0.013319201554495619\n",
      "False Positive Parity:  0.19845085192760947\n",
      "Equalized Odds:  0.2117700534821051\n",
      "\n",
      "\n",
      "\n",
      "__________________\n",
      "Iteration:  6\n",
      "Average Adversary batch loss:  0.460739283667709\n",
      "Adversary Mini-Batch loss:  0.30075356364250183\n",
      "Classifier Mini-Batch loss:  0.2964341342449188\n",
      "Total Mini-Batch loss:  -0.6058264970779419\n",
      "Iteration:  7\n",
      "Average Adversary batch loss:  0.4804428135514607\n",
      "Adversary Mini-Batch loss:  0.4759870171546936\n",
      "Classifier Mini-Batch loss:  0.4130595326423645\n",
      "Total Mini-Batch loss:  -1.0149016380310059\n",
      "Training metrics:\n",
      "F1 Score:  0.8390703636918622\n",
      "Precision Score:  0.7608958506279814\n",
      "Recall Score:  0.9951290110209319\n",
      "Acc Score:  0.842948250728863 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness Metrics on Train:\n",
      "Toxicity Prediction Rates:  Female - 0.81 Non-Female -  0.49\n",
      "True Positive Prediction Rates:  Female - 1.0 Non-Female -  0.99\n",
      "False Positive Prediction Rates:  Female - 0.36 Non-Female -  0.28\n",
      "Demographic Parity:  0.32033490311555707\n",
      "True Positive Parity:  0.0009226354706985962\n",
      "False Positive Parity:  0.07532631324069211\n",
      "Equalized Odds:  0.07624894871139071\n",
      "\n",
      "\n",
      "Validation metrics:\n",
      "F1 Score:  0.6449536867282797\n",
      "Precision Score:  0.29165284222548543\n",
      "Recall Score:  0.9621567145376668\n",
      "Acc Score:  0.5987103174603174 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness Metrics on Validation:\n",
      "Toxicity Prediction Rates:  Female - 0.6 Non-Female -  0.49\n",
      "True Positive Prediction Rates:  Female - 0.96 Non-Female -  0.95\n",
      "False Positive Prediction Rates:  Female - 0.53 Non-Female -  0.39\n",
      "Demographic Parity:  0.11015619894124562\n",
      "True Positive Parity:  0.007913796149090269\n",
      "False Positive Parity:  0.14128633875080282\n",
      "Equalized Odds:  0.1492001348998931\n",
      "\n",
      "\n",
      "\n",
      "__________________\n",
      "Iteration:  8\n",
      "Average Adversary batch loss:  0.521070582265409\n",
      "Adversary Mini-Batch loss:  0.45652270317077637\n",
      "Classifier Mini-Batch loss:  0.2973632216453552\n",
      "Total Mini-Batch loss:  -1.072204828262329\n",
      "Iteration:  9\n",
      "Average Adversary batch loss:  0.5330338545860424\n",
      "Adversary Mini-Batch loss:  0.37439751625061035\n",
      "Classifier Mini-Batch loss:  0.41200846433639526\n",
      "Total Mini-Batch loss:  -0.7111840844154358\n",
      "Training metrics:\n",
      "F1 Score:  0.903337912788948\n",
      "Precision Score:  0.8445365799227965\n",
      "Recall Score:  0.9869788863945552\n",
      "Acc Score:  0.9041241496598639 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness Metrics on Train:\n",
      "Toxicity Prediction Rates:  Female - 0.75 Non-Female -  0.41\n",
      "True Positive Prediction Rates:  Female - 0.99 Non-Female -  0.99\n",
      "False Positive Prediction Rates:  Female - 0.18 Non-Female -  0.18\n",
      "Demographic Parity:  0.3412891039218865\n",
      "True Positive Parity:  0.002076297686053774\n",
      "False Positive Parity:  0.006667112591950242\n",
      "Equalized Odds:  0.008743410278004016\n",
      "\n",
      "\n",
      "Validation metrics:\n",
      "F1 Score:  0.7383150624880973\n",
      "Precision Score:  0.3587262947409638\n",
      "Recall Score:  0.9246787603930462\n",
      "Acc Score:  0.6994047619047619 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness Metrics on Validation:\n",
      "Toxicity Prediction Rates:  Female - 0.47 Non-Female -  0.4\n",
      "True Positive Prediction Rates:  Female - 0.92 Non-Female -  0.93\n",
      "False Positive Prediction Rates:  Female - 0.38 Non-Female -  0.29\n",
      "Demographic Parity:  0.06694823867721061\n",
      "True Positive Parity:  0.009185656244479756\n",
      "False Positive Parity:  0.09459088636960322\n",
      "Equalized Odds:  0.10377654261408298\n",
      "\n",
      "\n",
      "\n",
      "__________________\n",
      "Iteration:  10\n",
      "Average Adversary batch loss:  0.5170622487109882\n",
      "Adversary Mini-Batch loss:  0.48087993264198303\n",
      "Classifier Mini-Batch loss:  0.18989059329032898\n",
      "Total Mini-Batch loss:  -1.2527492046356201\n",
      "Iteration:  11\n",
      "Average Adversary batch loss:  0.531700737100996\n",
      "Adversary Mini-Batch loss:  0.49297916889190674\n",
      "Classifier Mini-Batch loss:  0.19261108338832855\n",
      "Total Mini-Batch loss:  -1.2863264083862305\n",
      "Training metrics:\n",
      "F1 Score:  0.9255450972964808\n",
      "Precision Score:  0.8978137301191953\n",
      "Recall Score:  0.9559894697507693\n",
      "Acc Score:  0.9254616132167153 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness Metrics on Train:\n",
      "Toxicity Prediction Rates:  Female - 0.7 Non-Female -  0.36\n",
      "True Positive Prediction Rates:  Female - 0.95 Non-Female -  0.97\n",
      "False Positive Prediction Rates:  Female - 0.1 Non-Female -  0.11\n",
      "Demographic Parity:  0.3439024325423296\n",
      "True Positive Parity:  0.021064145483083774\n",
      "False Positive Parity:  0.010127376279963116\n",
      "Equalized Odds:  0.03119152176304689\n",
      "\n",
      "\n",
      "Validation metrics:\n",
      "F1 Score:  0.8084006701264989\n",
      "Precision Score:  0.4298033976605407\n",
      "Recall Score:  0.8748929201310154\n",
      "Acc Score:  0.7807539682539683 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness Metrics on Validation:\n",
      "Toxicity Prediction Rates:  Female - 0.35 Non-Female -  0.34\n",
      "True Positive Prediction Rates:  Female - 0.86 Non-Female -  0.88\n",
      "False Positive Prediction Rates:  Female - 0.25 Non-Female -  0.22\n",
      "Demographic Parity:  0.010865302921377662\n",
      "True Positive Parity:  0.010952128599187372\n",
      "False Positive Parity:  0.028354571573933868\n",
      "Equalized Odds:  0.03930670017312124\n",
      "\n",
      "\n",
      "\n",
      "__________________\n",
      "Iteration:  12\n",
      "Average Adversary batch loss:  0.5897279374801035\n",
      "Adversary Mini-Batch loss:  0.6921532154083252\n",
      "Classifier Mini-Batch loss:  0.25169607996940613\n",
      "Total Mini-Batch loss:  -1.824763536453247\n",
      "Iteration:  13\n",
      "Average Adversary batch loss:  0.5966860779694149\n",
      "Adversary Mini-Batch loss:  0.5472656488418579\n",
      "Classifier Mini-Batch loss:  0.2742132842540741\n",
      "Total Mini-Batch loss:  -1.3675836324691772\n",
      "Training metrics:\n",
      "F1 Score:  0.9207757294976954\n",
      "Precision Score:  0.9195820394198864\n",
      "Recall Score:  0.9191514762571293\n",
      "Acc Score:  0.9208090379008745 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness Metrics on Train:\n",
      "Toxicity Prediction Rates:  Female - 0.66 Non-Female -  0.33\n",
      "True Positive Prediction Rates:  Female - 0.91 Non-Female -  0.95\n",
      "False Positive Prediction Rates:  Female - 0.07 Non-Female -  0.08\n",
      "Demographic Parity:  0.32941408232852215\n",
      "True Positive Parity:  0.038083606175428275\n",
      "False Positive Parity:  0.0181147878409839\n",
      "Equalized Odds:  0.056198394016412176\n",
      "\n",
      "\n",
      "Validation metrics:\n",
      "F1 Score:  0.8376089827066993\n",
      "Precision Score:  0.4784855576522242\n",
      "Recall Score:  0.8184303350970016\n",
      "Acc Score:  0.8204365079365079 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness Metrics on Validation:\n",
      "Toxicity Prediction Rates:  Female - 0.28 Non-Female -  0.29\n",
      "True Positive Prediction Rates:  Female - 0.81 Non-Female -  0.83\n",
      "False Positive Prediction Rates:  Female - 0.18 Non-Female -  0.18\n",
      "Demographic Parity:  0.013798117770080387\n",
      "True Positive Parity:  0.024659954071718837\n",
      "False Positive Parity:  0.0015441048864022477\n",
      "Equalized Odds:  0.026204058958121085\n",
      "\n",
      "\n",
      "\n",
      "__________________\n",
      "Iteration:  14\n",
      "Average Adversary batch loss:  0.6173845522605296\n",
      "Adversary Mini-Batch loss:  0.5689623355865479\n",
      "Classifier Mini-Batch loss:  0.18931177258491516\n",
      "Total Mini-Batch loss:  -1.5175752639770508\n",
      "Iteration:  15\n",
      "Average Adversary batch loss:  0.636310566357899\n",
      "Adversary Mini-Batch loss:  0.6894822716712952\n",
      "Classifier Mini-Batch loss:  0.2757713496685028\n",
      "Total Mini-Batch loss:  -1.792675495147705\n",
      "Training metrics:\n",
      "F1 Score:  0.9120407888731471\n",
      "Precision Score:  0.9038021751133724\n",
      "Recall Score:  0.9190483551105796\n",
      "Acc Score:  0.911959426627794 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness Metrics on Train:\n",
      "Toxicity Prediction Rates:  Female - 0.66 Non-Female -  0.35\n",
      "True Positive Prediction Rates:  Female - 0.9 Non-Female -  0.95\n",
      "False Positive Prediction Rates:  Female - 0.07 Non-Female -  0.11\n",
      "Demographic Parity:  0.309017588293606\n",
      "True Positive Parity:  0.04878195931280582\n",
      "False Positive Parity:  0.039072746566626876\n",
      "Equalized Odds:  0.0878547058794327\n",
      "\n",
      "\n",
      "Validation metrics:\n",
      "F1 Score:  0.8338172981931977\n",
      "Precision Score:  0.47530441339965146\n",
      "Recall Score:  0.8400730662635425\n",
      "Acc Score:  0.8139880952380952 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness Metrics on Validation:\n",
      "Toxicity Prediction Rates:  Female - 0.28 Non-Female -  0.32\n",
      "True Positive Prediction Rates:  Female - 0.81 Non-Female -  0.85\n",
      "False Positive Prediction Rates:  Female - 0.18 Non-Female -  0.2\n",
      "Demographic Parity:  0.0403731782236455\n",
      "True Positive Parity:  0.04426779720897367\n",
      "False Positive Parity:  0.026525675008195165\n",
      "Equalized Odds:  0.07079347221716883\n",
      "\n",
      "\n",
      "\n",
      "__________________\n",
      "Iteration:  16\n",
      "Average Adversary batch loss:  0.6414620161230293\n",
      "Adversary Mini-Batch loss:  0.6866372227668762\n",
      "Classifier Mini-Batch loss:  0.20979037880897522\n",
      "Total Mini-Batch loss:  -1.8501213788986206\n",
      "Iteration:  17\n",
      "Average Adversary batch loss:  0.6429164875353043\n",
      "Adversary Mini-Batch loss:  0.5823472738265991\n",
      "Classifier Mini-Batch loss:  0.23828339576721191\n",
      "Total Mini-Batch loss:  -1.5087584257125854\n",
      "Training metrics:\n",
      "F1 Score:  0.9047139644941807\n",
      "Precision Score:  0.885096655319742\n",
      "Recall Score:  0.9292723766457184\n",
      "Acc Score:  0.9046950923226432 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness Metrics on Train:\n",
      "Toxicity Prediction Rates:  Female - 0.67 Non-Female -  0.37\n",
      "True Positive Prediction Rates:  Female - 0.91 Non-Female -  0.97\n",
      "False Positive Prediction Rates:  Female - 0.07 Non-Female -  0.14\n",
      "Demographic Parity:  0.29123083505858777\n",
      "True Positive Parity:  0.054958859090853296\n",
      "False Positive Parity:  0.06156374557536992\n",
      "Equalized Odds:  0.11652260466622322\n",
      "\n",
      "\n",
      "Validation metrics:\n",
      "F1 Score:  0.825851577336237\n",
      "Precision Score:  0.46186596368669197\n",
      "Recall Score:  0.8368795666414715\n",
      "Acc Score:  0.8025793650793651 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness Metrics on Validation:\n",
      "Toxicity Prediction Rates:  Female - 0.29 Non-Female -  0.34\n",
      "True Positive Prediction Rates:  Female - 0.81 Non-Female -  0.87\n",
      "False Positive Prediction Rates:  Female - 0.18 Non-Female -  0.23\n",
      "Demographic Parity:  0.05645055878700739\n",
      "True Positive Parity:  0.05847023494082315\n",
      "False Positive Parity:  0.043028759509743\n",
      "Equalized Odds:  0.10149899445056615\n",
      "\n",
      "\n",
      "\n",
      "__________________\n",
      "Iteration:  18\n",
      "Average Adversary batch loss:  0.6465973914017135\n",
      "Adversary Mini-Batch loss:  0.7877855896949768\n",
      "Classifier Mini-Batch loss:  0.4329259395599365\n",
      "Total Mini-Batch loss:  -1.9304308891296387\n",
      "Iteration:  19\n",
      "Average Adversary batch loss:  0.6464756342308167\n",
      "Adversary Mini-Batch loss:  0.7376344203948975\n",
      "Classifier Mini-Batch loss:  0.33587199449539185\n",
      "Total Mini-Batch loss:  -1.8770313262939453\n",
      "Training metrics:\n",
      "F1 Score:  0.9007493807312319\n",
      "Precision Score:  0.8675075167518635\n",
      "Recall Score:  0.9426334974937496\n",
      "Acc Score:  0.9008685617103984 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness Metrics on Train:\n",
      "Toxicity Prediction Rates:  Female - 0.68 Non-Female -  0.4\n",
      "True Positive Prediction Rates:  Female - 0.93 Non-Female -  0.98\n",
      "False Positive Prediction Rates:  Female - 0.08 Non-Female -  0.16\n",
      "Demographic Parity:  0.28224208221158575\n",
      "True Positive Parity:  0.04911991563354545\n",
      "False Positive Parity:  0.0817825910875212\n",
      "Equalized Odds:  0.13090250672106665\n",
      "\n",
      "\n",
      "Validation metrics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.8101646904543559\n",
      "Precision Score:  0.4256701463284097\n",
      "Recall Score:  0.8389014865205342\n",
      "Acc Score:  0.7827380952380952 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness Metrics on Validation:\n",
      "Toxicity Prediction Rates:  Female - 0.31 Non-Female -  0.37\n",
      "True Positive Prediction Rates:  Female - 0.82 Non-Female -  0.88\n",
      "False Positive Prediction Rates:  Female - 0.21 Non-Female -  0.26\n",
      "Demographic Parity:  0.06467714528462193\n",
      "True Positive Parity:  0.060731319554848895\n",
      "False Positive Parity:  0.05280512387696404\n",
      "Equalized Odds:  0.11353644343181293\n",
      "\n",
      "\n",
      "\n",
      "__________________\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#NEW IMPLEMENTATION WITH PRETRAINING\n",
    "\n",
    "# Training Process\n",
    "\n",
    "lambda_params = [3]\n",
    "\n",
    "lbda_train_accs = []\n",
    "lbda_valid_accs = []\n",
    "protected_toxicity_rates = []\n",
    "unprotected_toxicity_rates = []\n",
    "protected_tp_rates = []\n",
    "unprotected_tp_rates = []\n",
    "protected_fp_rates = []\n",
    "unprotected_fp_rates = []\n",
    "demo_parity_scores = []\n",
    "tp_parity_scores = []\n",
    "fp_parity_scores = []\n",
    "equ_odds_scores = []\n",
    "\n",
    "for lbda in lambda_params:\n",
    "\n",
    "    #DEFINING MODELS\n",
    "\n",
    "    clf = Classifier(toxicity_labels = 2) # instantiate the nn\n",
    "    adv = Adversary(identity_labels = 2)\n",
    "\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Defining optimizers\n",
    "    optimizer_adv = optim.Adam(adv.parameters(), lr=0.001)\n",
    "\n",
    "    lrlast = .001\n",
    "    lrmain = .00001\n",
    "    optimizer_clf = optim.Adam(\n",
    "        [\n",
    "            {\"params\":clf.bert.parameters(),\"lr\": lrmain},\n",
    "            {\"params\":clf.c1.parameters(), \"lr\": lrlast},\n",
    "            #{\"params\":clf.c2.parameters(), \"lr\": lrlast},\n",
    "        {\"params\":clf.c3.parameters(), \"lr\": lrlast}    \n",
    "      ])\n",
    "\n",
    "    clf.to(device)\n",
    "    adv.to(device)\n",
    "\n",
    "    #PRETRAIN CLASSIFIER\n",
    "\n",
    "    for param in adv.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    clf = pretrain_classifier(clf, optimizer_clf, train_loader, loss_criterion, 3)\n",
    "\n",
    "    for param in adv.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    #PRETRAIN ADVERSARY\n",
    "\n",
    "    for param in clf.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    adv = pretrain_adversary(adv, clf, optimizer_adv, train_loader, loss_criterion, 3)\n",
    "\n",
    "    for param in clf.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    print('Lambda: ' + str(lbda))\n",
    "\n",
    "    train_accs = []\n",
    "    valid_accs = []\n",
    "    iterations = 20\n",
    "\n",
    "    for iteration in range(iterations):  # loop over the dataset multiple times\n",
    "        print(\"Iteration: \", iteration)\n",
    "\n",
    "        #TRAIN ADVERSARY FOR 1 EPOCH\n",
    "\n",
    "        for param in clf.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        adv = train_adversary(adv, clf, optimizer_adv, train_loader, loss_criterion, epochs=1)\n",
    "\n",
    "        for param in clf.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        #TRAIN CLASSIFIER FOR 1 SAMPLE MINI BATCH\n",
    "\n",
    "        for param in adv.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        clf = train_classifier(clf, optimizer_clf, adv, train_loader, loss_criterion, lbda)\n",
    "\n",
    "        for param in adv.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        if (iteration + 1) % 2 == 0:\n",
    "          \n",
    "            print('Training metrics:')\n",
    "            y_pred, actual_labels, protected_labels, acc_score = conduct_validation(clf, train_loader, adv = True)\n",
    "            train_accs.append(acc_score)\n",
    "            print(\"\\n\")\n",
    "            print(\"Fairness Metrics on Train:\")\n",
    "            non_protected_labels = np.asarray(get_unprotected_class(protected_labels))\n",
    "            thres = 0.5\n",
    "            female_tox_rate, nf_tox_rate, female_tp_rate, nf_tp_rate, female_fp_rate, nf_fp_rate, demo_parity, tp_parity, fp_parity, equ_odds =\\\n",
    "            get_fairness_metrics(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
    "            print(\"Toxicity Prediction Rates: \", \"Female -\", female_tox_rate, \"Non-Female - \", nf_tox_rate)\n",
    "            print(\"True Positive Prediction Rates: \", \"Female -\", female_tp_rate, \"Non-Female - \", nf_tp_rate)\n",
    "            print(\"False Positive Prediction Rates: \", \"Female -\", female_fp_rate, \"Non-Female - \", nf_fp_rate)\n",
    "            print(\"Demographic Parity: \", demo_parity)\n",
    "            print(\"True Positive Parity: \", tp_parity)\n",
    "            print(\"False Positive Parity: \", fp_parity)\n",
    "            print(\"Equalized Odds: \", equ_odds)\n",
    "            print(\"\\n\")\n",
    "            print('Validation metrics:')\n",
    "            y_pred, actual_labels, protected_labels, acc_score = conduct_validation(clf, val_loader, adv = True)\n",
    "            valid_accs.append(acc_score)\n",
    "            \n",
    "            print(\"\\n\")\n",
    "            print(\"Fairness Metrics on Validation:\")\n",
    "            non_protected_labels = np.asarray(get_unprotected_class(protected_labels))\n",
    "            thres = 0.5\n",
    "            female_tox_rate, nf_tox_rate, female_tp_rate, nf_tp_rate, female_fp_rate, nf_fp_rate, demo_parity, tp_parity, fp_parity, equ_odds =\\\n",
    "            get_fairness_metrics(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
    "            print(\"Toxicity Prediction Rates: \", \"Female -\", female_tox_rate, \"Non-Female - \", nf_tox_rate)\n",
    "            print(\"True Positive Prediction Rates: \", \"Female -\", female_tp_rate, \"Non-Female - \", nf_tp_rate)\n",
    "            print(\"False Positive Prediction Rates: \", \"Female -\", female_fp_rate, \"Non-Female - \", nf_fp_rate)\n",
    "            print(\"Demographic Parity: \", demo_parity)\n",
    "            print(\"True Positive Parity: \", tp_parity)\n",
    "            print(\"False Positive Parity: \", fp_parity)\n",
    "            print(\"Equalized Odds: \", equ_odds)\n",
    "            print(\"\\n\\n\\n__________________\")\n",
    "            if iteration == iterations -1:\n",
    "                protected_toxicity_rates.append(female_tox_rate)\n",
    "                unprotected_toxicity_rates.append(nf_tox_rate)\n",
    "                protected_tp_rates.append(female_tp_rate)\n",
    "                unprotected_tp_rates.append(nf_tp_rate)\n",
    "                protected_fp_rates.append(female_fp_rate)\n",
    "                unprotected_fp_rates.append(nf_fp_rate)\n",
    "                demo_parity_scores.append(demo_parity)\n",
    "                tp_parity_scores.append(tp_parity)\n",
    "                fp_parity_scores.append(fp_parity)\n",
    "                equ_odds_scores.append(equ_odds)\n",
    "    lbda_train_accs.append(train_accs)  \n",
    "    lbda_valid_accs.append(valid_accs)\n",
    "\n",
    "    # del clf\n",
    "    # del adv\n",
    "\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vqu2IJ1m297A"
   },
   "outputs": [],
   "source": [
    "torch.save(clf.state_dict(), path + \"SC_Classifier_Final_AllData\")\n",
    "torch.save(adv.state_dict(), path + \"SC_Adversary_Final_AllData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8TXgaHj529-2"
   },
   "outputs": [],
   "source": [
    "del adv\n",
    "del clf\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RQUlk0P42-FA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDE3ErIF2-IZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q_34yor52-Lu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yp_bNF0m2-PO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Adyp9y3_MXMS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Debiased_Classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
